对周志华的机器学习（西瓜书）的一些备忘

1. 绪论
 
    - 归纳偏好

        > 任何机器学习都有对某种类型假设的偏好

        > 一般性原则：奥卡姆剃刀，如果有多个假设与观察一致，选择最简单的。

        > 总误差与学习算法无关，称为“没有免费的午餐”定理
        
        > 学习算法的相对优劣需要针对具体的问题，需要归纳偏好与问题相匹配。

    -  多释原则，保留与经验观察一致的所有假设；这个和集成学习的研究相吻合。

2. 模型评估与选择

    - 经验误差与过拟合
    
        > m个样本中有a个分类错误：错误率=a/m、精度=1-a/m
        
        > 误差，训练误差/经验误差，新样本上：泛化误差。
        
        > 过拟合、欠拟合

    - 评估方法
       
       > 留出法 数据集D 分为 训练集S 和 测试集T  S,T 互斥 

       > 交叉验证法 将数据集分为k个大小相同的子集，然后k-1为训练集，1为测试集

       > 自助法 将数据集D随机重复采样m次得到D'，这样会有38%的样本没有被采样，将 D' 作为训练集， D\D' 作为测试集。

    - 性能度量
       
       > 回归，性能度量为均方误差

       > 查准率、查全率、F1

       > TP 真正例 FP 假正例    FN 假反例 TN 真反例

       > 查准率 P = TP / (TP+FP) 查全率 R = TP / (TP+FN)

       > P-R 曲线 差准率为纵轴，查全率为横轴；

       > 如果一个学习器的P-R曲线被另一个包住，则后者优于前者 

       > 平衡点：查准率=查全率

       > F1 度量： F1 = 2 x P x R /(P+R) = 2xTP/(样本总数+TP-TN)

       > Fb = (1+b^2)*P*R/(b^2*P+R) b>0 查全率更重要， b<0 查准率更重要， b=1 ,Fb=F1

       > ROC , 纵轴 TPR = TP/(TP+FN) ，横轴 FPR=FP/(TN+FP)

       > ROC 和 P-R 曲线一样，如果一个学习器的P-R曲线被另一个包住，则后者优于前者

       > AUC ROC曲线下的面积

       > 非均等代价，代价曲线

    - 比较检验

       > 泛化错误率

       > 二项检验

       > 交叉验证t检验

       > McNemar 检验， Friedman 检验

       > 泛化误差可分解为偏差、方差和噪声之和

3. 线性模型
    - 基本形式
        > $$ f(x) = w_1*x_1 + w_2x_2 + ... + w_dx_d + b $$
        > $$ f(x) = w^tx + b $$

    - 线性回归
        > 欧式距离/均方误差 --> 最小二乘法
        
        > $$ (w^*,b^*)=argmin_{(w,b)}\sum^{m}_{i=1}(f(x_i)-y_i)^2 $$ 
        > $$ = argmin_{(w,b)}\sum^{m}_{i=1}(y_i-wx_i-b)^2 $$

        > 对 w 和 b 求导

        > $$ w =\dfrac{\sum^{m}_{i=1}y_{1}\left( x_{i}-\overline {x}\right) }{\sum^{m}_{i=1}x^{2}_{1}-\dfrac {1}{m}\left( \sum^{m}_{i=1}x_{i}\right) ^{2}} $$ 
    
        > $$ b=\dfrac{1}{m}\sum^m_{i=1}(y_i-wx_i) $$

        > $$ \overline x = \dfrac1m\sum^m_{i=1}x_i $$
        
        > 多元线性回归

        > $$ f(x_i) = w^Tx_i+b \rightarrow f(x_i) \simeq y_i $$

        > 对数线性回归

        > $$ lny=w^tx+b $$
        > $$ y = e^{w^tx + b} $$

        > 对数几率回归 

        > sigmoid 函数

        > $$ y = \dfrac {1}{1+e^{-z}} $$

        > $$ y = \dfrac {1}{1+e^{-(w^tx+b)}} $$

        > $$ ln\dfrac {y}{1-y} = w^tx+b $$

        > y 为 x 的正例可能性 1-y 为 x 的反例可能性

        > 几率

        > $$ \dfrac {y}{1-y} $$

        > 对数几率

        > $$ ln\dfrac {y}{1-y} $$

        > 极大似然法估算wb，采用梯度下降或牛顿法求最优解

    - 线性判别分析

        > 给定训练样本，将样本投射到一条直线上，使得同类样本的投影点尽可能的接近，异类的投影点尽可能远离

    - 多分类学习

        > 一般用拆解法，将多分类拆解为多个二分类，分为正类和其他类

        > OvO OvR MvM ECOC

    - 类别不平衡问题
        
        > 欠采样，过采样

        > 阈值移动

        > $$ \dfrac {y'}{1-y'} = \dfrac {y}{1-y} * \dfrac {m^-}{m^+} $$

    - 阅读材料

        > 稀疏表示

        > 多标记学习


 4. 决策树       

    - 划分选择
        > 信息增益

        > 信息熵 样本D中第k类样本所占的比例为 p<sub>k</sub> (k=1,2,...,|y|), 则D的信息熵定义为：

        > $$Ent(D) = - \sum ^{|y|}_{k=1} p_klog_2p_k $$

        > Ent(D) 的值越少，D的纯度越高

        > 西瓜数据集 17 个样本，2个分类（好瓜、怀瓜），正例8个，反例9个，求 信息熵

        > $$ Ent(D) = - \sum ^2_1 p_klog_2p_k = -(\dfrac {8}{17}log_2\dfrac {8}{17} + \dfrac {9}{17}log_2\dfrac {9}{17}) = 0.998 $$

        > 信息增益越大，则使用属性a来划分所获得的纯度提升越大。可以用信息增益来进行决策树的划分属性

        > 增益率对可取值少的属性有偏好，因此应该先找到信息增益大于平均值的划分属性，然后再选增益率最高的

        > 基尼指数越小，数据的纯度越高

    - 剪枝处理

        > 防止过拟合

    - 连续和缺失值

        > 连续值直接用二分法处理，计算信息增益

        > 缺失值，先默认权重为1，然后缺失的数据直接并入各子节点，同时修改各子节点的权重

    - 多变量决策树

        >  直接用线性分类器

5. 神经网络

    - 神经元模型

        > M-P 神经元模型

        > $$ y = f(\sum_{i=1}^{n}w_ix_i-\theta)$$

        > 激活函数，有阶跃函数 Sigmoid

        > $$ sgn(x)=\begin{cases}1,x\geq 0\\ 0,x <0\end{cases} $$

        > $$ sigmoid(x)=\dfrac 1{1+e^{-e}} $$

    - 感知机与多层网络

        > 感知机由两层神经元组成